---
title: "Tutorial: Machine Learning Recap"
author: Linda T. Betz, M.Sc.
date: June 29, 2021
output: 
  beamer_presentation:
    theme: "Boadilla"
    colortheme: "seahorse"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})


library(tidyverse)
library(mlr)

```

# Structure of the tutorial
- June 28: Introduction to R & Data Wrangling
- June 29 (today): Recap of ML concepts, Hands-On ML Analyses
- June 30: Hands-On ML Analyses
- July 1: Hands-On ML Analyses

# Today's tutorial ...
... is based on the documentation of the mlr package (https://mlr.mlr-org.com/) and the interactive lecture "Introduction to Machine Learning" (https://introduction-to-machine-learning.netlify.app/).

# What is Machine Learning?
![](./imgs/ML_overview.PNG){ width=80% height=80% }


# Machine Learning in a nutshell
We typically have 3 components:

- a task (i.e., the ML problem)
- a learner (i.e., a specific ML algorithm)
- an optimization/tuning scheme

# Tasks
Tasks contain the data set and other relevant information about a machine learning problem, for example the name of the target variable for supervised problems.

Different types of tasks (depending on the problem): classification tasks, regression tasks, survival analysis tasks, cluster analysis tasks...


# Tasks
Example: binary classification task for breast cancer (benign vs. malignant)
```{r, include=TRUE, size="tiny"}
data(BreastCancer, package = "mlbench") # load data
classif.task <- makeClassifTask(id = "BreastCancer", data = BreastCancer %>% select(-Id), target = "Class")
classif.task
```

# Learner
 A learner is a ML algorithm. Most of the popular algorithms can generally be used for both classification and regression (e.g. SVM, Random Forests, kNN, ...).
```{r, include=TRUE, size="tiny"}
# classification SVM, set it up for predicting probabilities
classif.lrn <- makeLearner("classif.ksvm", predict.type = "prob")
classif.lrn
```

# Quiz
- via Zoom

# Tuning hyperparameters
Many machine learning algorithms have hyperparameters that need to be set. Often suitable values are not obvious and it is preferable to "tune" the hyperparameters, i.e., to automatically identify values that lead to the best performance.

To tune a machine learning algorithm, you have to specify:

- the search space, i.e., the range of values of the hyperparameter(s) you want to cover;
- the optimization algorithm (a.k.a. tuning method);
- an evaluation method, i.e., a resampling strategy and a performance measure.

# Tuning hyperparameters
```{r, include=TRUE}
# create a search space for the C hyperparameter (SVM)
search_space <- makeParamSet(
  makeNumericParam("C", lower = 0.01, upper = 0.1)
)

# random search with 100 iterations
ctrl <- makeTuneControlRandom(maxit = 100)

# evaluation method could be 3-fold CV 
resample <- makeResampleDesc("CV", iters = 3L)

# use balanced accuravy (BAC) as performance measure
measure <- bac
```
# Quiz
- via Zoom


# Model parameters vs. hyperparameters
Don't confuse model parameters with hyperparameters.
Model parameters are optimized during training, typically via loss minimization. They are an *output* of the training.

Examples:

- coefficients $\theta$ of a linear model f(x) = $\theta$$^T$x 
- the splits and terminal node constants of a decision tree

Hyperparameters, in contrast, are an *input* of the training, i.e., they must be specified before. Hyperparameters often control the complexity of a model.

Examples:

- the number and maximal order of interactions to be included in a linear model
- the maximum depth of a decision tree

# Model parameters vs. hyperparameters

![](./imgs/Resampling_tuning.png){ width=100% height=100% }


# Resampling
Instead of using a single train/validation split for tuning the hyperparameters, we can repeat the process several times to get more stable results. This resampling can have different structural forms, such as cross-validation, bootstrapping, subsampling, etc. Repeating the resampling process can improve error estimation for small sample sizes.

Example: 4-fold cross-validation

![](./imgs/Tuning.png){ width=75% height=75% }

# Train - validate - test
After optimizing the hyperparameters, we typically want to evaluate our model. Evaluation on data that was part of the training process leads to overly optimistic performance estimates. For unbiased performance estimates, we need a truly "untouched" test data set only used once after a model is completely trained. 

![](./imgs/train_validate_test.png){ width=90% height=90% }

# Nested resampling
As with hyperparameter tuning, model evaluation can be repeated. Hence, we end up with "nested resampling" - outer resampling for model evaluation and inner resampling for hyperparameter optimization. 

![](./imgs/nested_resampling.png){ width=90% height=90% }

# Quiz
- via Zoom

# Any questions?
![](./imgs/questions.jpg){ width=75% height=75% }

# References
- Documentation of the mlr package (https://mlr.mlr-org.com/)
- Interactive lecture "Introduction to Machine Learning" (https://introduction-to-machine-learning.netlify.app/).
- Photos: https://unsplash.com/

